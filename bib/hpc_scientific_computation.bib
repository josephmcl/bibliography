@article{balay1998petsc, 
    Author = {Satish Balay and William~D. Gropp and Lois Curfman 
        McInnes and Barry~F. Smith},
    Title = {Efficient Management of Parallelism in Object Oriented 
        Numerical Software Libraries},
    Abstract = {Parallel numerical software based on the message 
        passing model is enormously complicated. This paper introduces 
        a set of techniques to manage the complexity, while maintaining 
        high efficiency and ease of use. The PETSc 2.0 package uses 
        object-oriented programming to conceal the details of the 
        message passing, without concealing the parallelism, in a 
        high-quality set of numerical software libraries. In fact, the 
        programming model used by PETSc is also the most appropriate 
        for NUMA shared-memory machines, since they require the same 
        careful attention to memory hierarchies as do 
        distributed-memory machines. Thus, the concepts discussed are 
        appropriate for all scalable computing systems. The PETSc 
        libraries provide many of the data structures and numerical 
        kernels required for the scalable solution of PDEs, offering 
        performance portability.},
    BookTitle = {Modern Software Tools in Scientific Computing},
    Editor = {E. Arge and A.~M. Bruaset and H.~P. Langtangen},
    Publisher = {Birkh{\"{a}}user Press},
    Pages = {163--202},
    Year = {1997}}

@MISC{gael2010eigen,
    Author = {Ga\"{e}l Guennebaud and Beno\^{i}t Jacob and others},
    Title = {Eigen v3},
    HowPublished = {http://eigen.tuxfamily.org},
    Year = {2010}}

@article{qian2013blas,
    Title = {AUGEM: Automatically Generate High Performance Dense 
        Linear Algebra Kernels on x86 CPUs},
    Abstract = {Basic Liner algebra subprograms (BLAS) is a fundamental 
    library in scientific computing. In this paper, we present a 
    template-based optimization framework, AUGEM, which can 
    automatically generate fully optimized assembly code for several 
    dense linear algebra (DLA) kernels, such as GEMM, GEMV, AXPY and 
    DOT, on varying multi-core CPUs without requiring any manual 
    interference from developers. In particular, based on 
    domain-specific knowledge about algorithms of the DLA kernels, we 
    use a collection of parameterized code templates to formulate a 
    number of commonly occurring instruction sequences within the 
    optimized low-level C code of these DLA kernels. Then, our 
    framework uses a specialized low-level C optimizer to identify 
    instruction sequences that match the pre-defined code templates 
    and thereby translates them into extremely efficient SSE/AVX 
    instructions. The DLA kernels generated by our template-based 
    approach surpass the implementations of Intel MKL and AMD ACML 
    BLAS libraries, on both Intel Sandy Bridge and AMD Piledriver 
    processors.},
    Author = {Qian, Wang and Xianyi, Zhang and Yunquan, Zhang and Yi, 
        Qing},
    Location = {Denver CO},
    Year = {2013},
    Publisher = {},
    BookTitle = {In the International Conference for High Performance 
        Computing, Networking, Storage and Analysis},
    Publisher = {Association for Computing Machinery},
    Series = {SC '13}}


@incollection{dongarra2014slate,
    Title = {Accelerating numerical dense linear algebra calculations 
        with GPUs},
    Abstract = {This chapter presents the current best design and 
        implementation practices for the acceleration of dense linear 
        algebra (DLA) on GPUs. Examples are given with fundamental 
        algorithms – from the matrix-matrix multiplication kernel 
        written in CUDA to the higher level algorithms for solving 
        linear systems, eigenvalue and SVD problems. The 
        implementations are available through the MAGMA library – a 
        redesign for GPUs of the popular LAPACK. To generate the 
        extreme level of parallelism needed for the efficient use of 
        GPUs, algorithms of interest are redesigned and then split 
        into well-chosen computational tasks. The tasks execution is 
        scheduled over the computational components of a hybrid system 
        of multi- core CPUs with GPU accelerators using either static 
        scheduling or a light-weight runtime system. The use of 
        light-weight runtime systems keeps scheduling overhead low, 
        similar to static scheduling, while enabling the expression of 
        parallelism through sequential-like code. This simplifies the 
        development effort and allows the exploration of the unique 
        strengths of the various hardware components.},
    Author = {Dongarra, Jack and Gates, Mark and Haidar, Azzam and 
        Kurzak, Jakub and Luszczek, Piotr and Tomov, Stanimire and 
        Yamazaki, Ichitaro},
    BookTitle = {Numerical computations with GPUs},
    Pages = {3--28},
    Year = {2014},
    Publisher = {Springer}}



    